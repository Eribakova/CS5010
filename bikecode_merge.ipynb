{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bikecode_merge.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IvzdIxnD5J1"
      },
      "source": [
        "This program 1.  merges multiple monthly bikeshare files, 2.  Checks for inconsistent, missing and duplicate data, 3. fills in key missing values, 4.  drops unnecessary data, and 5.  converts to date time formats.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlqzwgQEhCXP",
        "outputId": "53c0c26f-31ae-48f5-a68f-38bba23f0da7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVc5PEKikexj",
        "outputId": "a9d04ce1-e119-454e-9c42-4fc5f89d905f"
      },
      "source": [
        "dfa = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/201902-capitalbikeshare-tripdata.csv')\n",
        "dfb = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/201903-capitalbikeshare-tripdata.csv')\n",
        "dfc = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/201904-capitalbikeshare-tripdata.csv')\n",
        "dfd = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/201905-capitalbikeshare-tripdata.csv')\n",
        "dfe = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/201906-capitalbikeshare-tripdata.csv')\n",
        "dff = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/201907-capitalbikeshare-tripdata.csv')\n",
        "dfg = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/201908-capitalbikeshare-tripdata.csv')\n",
        "dfh = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/201909-capitalbikeshare-tripdata.csv')\n",
        "dfi = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/201910-capitalbikeshare-tripdata.csv')\n",
        "dfj = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/201911-capitalbikeshare-tripdata.csv')\n",
        "dfk = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/201912-capitalbikeshare-tripdata.csv')\n",
        "dfl = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/202001-capitalbikeshare-tripdata.csv')\n",
        "dfm = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/202002-capitalbikeshare-tripdata.csv')\n",
        "dfn = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/202003-capitalbikeshare-tripdata.csv')\n",
        "dfo = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/202005-capitalbikeshare-tripdata.csv')\n",
        "dfp = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/202006-capitalbikeshare-tripdata.csv')\n",
        "dfq = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/202007-capitalbikeshare-tripdata.csv')\n",
        "dfr = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/202008-capitalbikeshare-tripdata.csv')\n",
        "dfs = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/202009-capitalbikeshare-tripdata.csv')\n",
        "dft = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/202010-capitalbikeshare-tripdata.csv')\n",
        "dfu = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/202011-capitalbikeshare-tripdata.csv')\n",
        "dfv = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/202012-capitalbikeshare-tripdata.csv')\n",
        "dfw = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/202101-capitalbikeshare-tripdata.csv')\n",
        "dfx = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/202102-capitalbikeshare-tripdata.csv')\n",
        "dfy = pd.read_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/202103-capitalbikeshare-tripdata.csv')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (5,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmWO-7Fq0vUk"
      },
      "source": [
        "Consistency check 1:  \n",
        "Column headers.  Note column names and contents for each file.  There are different column headings for Feb 2019-March 2020 and May 2020 - March 2021.  The fields, \"rideable type\" and latitude / longitude are not available before May 2020. \"Duration\" is not availalbe after May 2020.  Note that there is no file for April 2020."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjn4Cv5X06aY"
      },
      "source": [
        "frames=[dfa,dfb,dfc,dfd,dfe,dff,dfg,dfh,dfi,dfj,dfk,dfl,dfm,dfn,dfo,dfp,dfq,dfr,dfs,dft,dfu,dfv,dfw,dfx,dfy]\n",
        "filenum=0\n",
        "for f in frames:\n",
        "  print(filenum, \":\\t\",f.columns)\n",
        "  filenum=filenum+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkYmy8iY0TMV"
      },
      "source": [
        "Consistency check 2:  Data types.\n",
        "Data types differ across files for a given variable. Biggest difference is station id can be int, float or object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en27_X6m03MC"
      },
      "source": [
        "frames=[dfa,dfb,dfc,dfd,dfe,dff,dfg,dfh,dfi,dfj,dfk,dfl,dfm,dfn,dfo,dfp,dfq,dfr,dfs,dft,dfu,dfv,dfw,dfx,dfy]\n",
        "num=0\n",
        "for f in frames:  #check dtypes\n",
        "    print(num)\n",
        "    for i in f.columns:\n",
        "      print(i,f[i].dtypes)\n",
        "    num=num+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9Rop_t62IDQ"
      },
      "source": [
        "Consistency check 3:  Formatting of dates and strings from pre-May 2020 to after May 2020.  \n",
        "There are some inconsistencies how the data are formatted in files before May 2020 and after.  However the dates and addresses are consistent from file to file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMdDEMjh2UoD"
      },
      "source": [
        "#March 2020 vs. May 2020\n",
        "#membership type\n",
        "print(\"membership type\\n\")\n",
        "print(\"March 2020: \", dfn['Member type'].value_counts())\n",
        "print(\"May 2020: \", dfo['member_casual'].value_counts())  #capitalization differs\n",
        "#Dates\n",
        "print(\"\\n dates\\n\")\n",
        "print(\"March 2020: \", dfn['Start date'].head())\n",
        "print(\"May 2020: \", dfo['started_at'].tail())  #no difference between the two time periods.\n",
        "#Ride id / bike number\n",
        "print(\"\\n bike id\\n\")\n",
        "print(\"March 2020: \", dfn['Bike number'].head())\n",
        "print(\"May 2020: \", dfo['ride_id'].head())   # very different format but of little consequence for analysis.\n",
        "#Station number\n",
        "print(\"\\n station id\\n\")\n",
        "print(\"March 2020: \", dfn['Start station number'].head())\n",
        "print(\"May 2020: \", dfo['start_station_id'].tail())   # No apparent difference\n",
        "#Station address / namef\n",
        "print(\"\\n station name\\n\")\n",
        "print(\"March 2020: \", dfn['Start station'].tail())\n",
        "print(\"May 2020: \", dfo['start_station_name'].tail())   # No apparent difference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaTZJpuWzxcR"
      },
      "source": [
        "Missing value check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h766CYODY_J1"
      },
      "source": [
        "num2=0 #Check number and percent of missings\n",
        "for f in frames:  \n",
        "  print(num2)\n",
        "  for i in f.columns:\n",
        "    print(i,\":\",f[i].isnull().sum(),round(f[i].isnull().sum()/f.shape[0],5) )\n",
        "  num2=num2+1\n",
        "\n",
        "# Conclusion:  Virtually no missing values before March 2020.  \n",
        "# After March 2020, station names and ids are increasingly missing. In these\n",
        "# cases, lat-long is almost always available. To fill in missing data about \n",
        "# about location we will need to impute based on latitude and longitude to know the location."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkxhKwqXARWL"
      },
      "source": [
        "Concatenate files with standard column headers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXUEvTZUkhPq",
        "outputId": "c8a9ca5b-1143-4de7-f59c-a0857a55894a"
      },
      "source": [
        "\n",
        "# Standardize columns which differ between pre-May 2020 and post-May 2020\n",
        "# Note that there is no April file.  \n",
        "# Format may2020 - present  \n",
        "# Index(['ride_id', 'rideable_type', 'started_at', 'ended_at',\n",
        "#        'start_station_name', 'start_station_id', 'end_station_name',\n",
        "#        'end_station_id', 'start_lat', 'start_lng', 'end_lat', 'end_lng',\n",
        "#        'member_casual'],\n",
        "\n",
        "\n",
        "frames=[dfa,dfb,dfc,dfd,dfe,dff,dfg,dfh,dfi,dfj,dfk,dfl,dfm,dfn,dfo,dfp,dfq,dfr,dfs,dft,dfu,dfv,dfw,dfx,dfy]\n",
        "      \n",
        "for f in frames:  #Renames columns as required\n",
        "    #del f['Duration']\n",
        "    f.rename(columns = {'Start date':'started_at', \n",
        "                              'End date':'ended_at',\n",
        "                              'Start station':'start_station_name',\n",
        "                              'Start station number':'start_station_id',\n",
        "                              'End station number':'end_station_id',\n",
        "                              'End station':'end_station_name',\n",
        "                              'Bike number':'ride_id',\n",
        "                              'Member type':'member_casual'\n",
        "                              }, inplace = True)\n",
        "  \n",
        "  #Create new variables and impose constisent dtype and formatting\n",
        "    f['fileid']=f['started_at'].str[0:7].astype(str)\n",
        "    f['member_casual']=f['member_casual'].str.lower().astype(str)\n",
        "    f['start_station_name']=f['start_station_name'].astype(str).str.lower()\n",
        "    f['end_station_id']\n",
        "    f['end_station_name']=f['end_station_name'].astype(str).str.lower()\n",
        "    \n",
        "    \n",
        "df = pd.concat(frames) #Puts all DF together\n",
        "\n",
        "df[\"id\"] = df.index + 1 #Create unique stable id for each observation\n",
        "print(df.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Duration', 'started_at', 'ended_at', 'start_station_id',\n",
            "       'start_station_name', 'end_station_id', 'end_station_name', 'ride_id',\n",
            "       'member_casual', 'fileid', 'rideable_type', 'start_lat', 'start_lng',\n",
            "       'end_lat', 'end_lng', 'id'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqhieq_kWAQ1"
      },
      "source": [
        "Check to make sure the items concatenated correctly by column header."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsefbjTr_Asp"
      },
      "source": [
        "Duplicate record check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj7gZZ7K_lT1"
      },
      "source": [
        "dup=df.duplicated()\n",
        "print(\"There are\", dup.sum(),\"duplicate records.\")\n",
        "#Conclusion: No dups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk9ijOGbBdIW"
      },
      "source": [
        "Clean latitude and longitude and fill in missing vals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfCFeHmsyCSi",
        "outputId": "f3b483e9-13db-405f-b2f1-4946607c4200"
      },
      "source": [
        "df = df[df[\"start_station_id\"] != 'MTL-ECO5-03']\n",
        "temp = df[[\"start_station_id\",\"start_lat\",\"start_lng\"]]\n",
        "temp[\"start_station_id\"] = temp[\"start_station_id\"]\n",
        "df_start_lat_lng = temp[temp[\"start_lat\"] != 'nan']\n",
        "df_start_lat_lng[\"start_station_id\"] = df_start_lat_lng[\"start_station_id\"].astype(float).astype(str)\n",
        "df_start_lat_lng[\"start_station_id\"] = df_start_lat_lng[\"start_station_id\"].str.split('.').str[0]\n",
        "df_start_lat_lng = df_start_lat_lng.groupby(\"start_station_id\").mean().reset_index()\n",
        "\n",
        "df = df[df[\"end_station_id\"] != 'MTL-ECO5-03']\n",
        "temp = df[[\"end_station_id\",\"end_lat\",\"end_lng\"]]\n",
        "temp[\"end_station_id\"] = temp[\"end_station_id\"]\n",
        "df_end_lat_lng = temp[temp[\"end_lat\"] != 'nan']\n",
        "df_end_lat_lng[\"end_station_id\"] = df_end_lat_lng[\"end_station_id\"].astype(float).astype(str)\n",
        "df_end_lat_lng[\"end_station_id\"] = df_end_lat_lng[\"end_station_id\"].str.split('.').str[0]\n",
        "df_end_lat_lng = df_end_lat_lng.groupby(\"end_station_id\").mean().reset_index()\n",
        "\n",
        "#Create a dictionary to start to fill in blank values\n",
        "dict_start_lat_lng = df_start_lat_lng.set_index('start_station_id').T.to_dict('list')\n",
        "dict_end_lat_lng = df_end_lat_lng.set_index('end_station_id').T.to_dict('list')\n",
        "\n",
        "dict_start_lat_lng\n",
        "dict_end_lat_lng\n",
        "\n",
        "def fillLatNa(row):\n",
        "    if row[\"start_station_id\"] == 'nan':\n",
        "        return [row[\"start_lat\"], row[\"start_lng\"]]\n",
        "    elif str(row[\"start_station_id\"]).split('.')[0] in dict_start_lat_lng.keys():\n",
        "        return dict_start_lat_lng[str(row[\"start_station_id\"]).split('.')[0]]\n",
        "    else:\n",
        "        print(row)\n",
        "\n",
        "lat_long_apply = df.apply(fillLatNa, axis=1)\n",
        "\n",
        "lat_long_apply\n",
        "\n",
        "def fillLatNaEnd(row):\n",
        "    if row[\"end_station_id\"] == 'nan':\n",
        "        return [row[\"end_lat\"], row[\"end_lng\"]]\n",
        "    elif str(row[\"end_station_id\"]).split('.')[0] in dict_end_lat_lng.keys():\n",
        "        return dict_end_lat_lng[str(row[\"end_station_id\"]).split('.')[0]]\n",
        "    else:\n",
        "        print(row)\n",
        "\n",
        "lat_long_end_apply = df.apply(fillLatNa, axis=1)\n",
        "\n",
        "\n",
        "lat_long_end_apply\n",
        "\n",
        "df[\"temp_start_lat\"] = lat_long_apply.str[0]\n",
        "df[\"temp_start_lng\"] = lat_long_apply.str[1]\n",
        "df[\"temp_end_lat\"] = lat_long_end_apply.str[0]\n",
        "df[\"temp_end_lng\"] = lat_long_end_apply.str[1]\n",
        "\n",
        "df[\"start_station_id\"] = df[\"start_station_id\"].astype(str)\n",
        "sorted_df = df.sort_values(by=[\"start_station_id\"])\n",
        "\n",
        "sorted_df[(sorted_df[\"start_lat\"].isnull() == False)]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Duration\n",
            "started_at\n",
            "ended_at\n",
            "start_station_id\n",
            "start_station_name\n",
            "end_station_id\n",
            "end_station_name\n",
            "ride_id\n",
            "member_casual\n",
            "fileid\n",
            "year\n",
            "month\n",
            "rideable_type\n",
            "start_lat\n",
            "start_lng\n",
            "end_lat\n",
            "end_lng\n",
            "id\n",
            "temp_start_lat\n",
            "temp_start_lng\n",
            "temp_end_lat\n",
            "temp_end_lng\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ49DyIuE32X"
      },
      "source": [
        "Consolidating files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX41U2OWyPkI",
        "outputId": "35d32065-45b5-42b9-a1a8-d66967bee885"
      },
      "source": [
        "#find nulls and dups\n",
        "dup=df.duplicated()\n",
        "print(\"There are\", dup.sum(),\"duplicate records.\")\n",
        "\n",
        "#2. The percentage of missing data in each column.\n",
        "for i in df.columns:\n",
        "  pct=df[i].isnull().sum()/df.shape[0]\n",
        "  print(i,\":\",str(round(100*pct,1))+\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 0 duplicate records.\n",
            "Duration : 34.0%\n",
            "started_at : 0.0%\n",
            "ended_at : 0.0%\n",
            "start_station_id : 2.1%\n",
            "start_station_name : 2.1%\n",
            "end_station_id : 2.3%\n",
            "end_station_name : 2.3%\n",
            "ride_id : 0.0%\n",
            "member_casual : 0.0%\n",
            "fileid : 0.0%\n",
            "year : 0.0%\n",
            "month : 0.0%\n",
            "rideable_type : 66.0%\n",
            "start_lat : 66.0%\n",
            "start_lng : 66.0%\n",
            "end_lat : 66.1%\n",
            "end_lng : 66.1%\n",
            "id : 0.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACAp1HgPExfM"
      },
      "source": [
        "# dropping unnecessary columns\n",
        "df = df.drop(columns=['ride_id','rideable type'])\n",
        "\n",
        "for col in df.columns:\n",
        "    print(col)\n",
        "    \n",
        "#Write to CSV\n",
        "df.to_csv('/content/drive/MyDrive/CS5010Project/Bikeshare_Data/full.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}